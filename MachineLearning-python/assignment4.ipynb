{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Python\n",
    "\n",
    "\n",
    "For this course, we are going to use Jupyter notebook as our environment for developing Python code.\n",
    "refer to https://jupyter.readthedocs.io/en/latest/content-quickstart.html on the instructions how to install it, the easiest way is to install from Anaconda (https://www.anaconda.com/download/) website, make sure you install (Version 4.4.0, Release Date: May 31, 2017) with Python 3.6.\n",
    "\n",
    "Also, it is good for the students who are not familiar with python (or they need a quick refreshment) to follow Jim Bagrow tutorial http://bagrow.com/ds1/whirlwindtourpython/00-Title.html. \n",
    "\n",
    "All the assignments to be written in Python 3.6 and can be run using Jupyter on one of the following Internet browsers (Chrome, Safari or Firefox), these are the browsers that officially supported by jupyter.\n",
    "\n",
    "<u> Note: for this assignment, submit your local copy of this page, running on IPython. Submit the file to Blackboard under Assignment1 using this file format:</u> <b>Yourfirstname_lastname_Assignment4.ipynb</b> \n",
    "\n",
    "#### <b>Deadline</b>: <u>Monday, Dec-04-2017 11:59 PM.</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 4\n",
    "\n",
    "In assignment 4 use tensor flow to build and train neural network architectures in part1 and part2.\n",
    "Hint: Refer to the MLP.ipynb from the shared documents on the black borad as an example to build and train a neural network using tensor flow from scratch.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 1\n",
    "In this part you are going to train a neural network on notMNIST dataset. The notMNIST dataset is a image recognition dataset of font glypyhs for the letters A through J useful with simple neural networks. It is quite similar to the classic MNIST dataset of handwritten digits 0 through 9. to make easy for you. the code below will load the notMNIST dataset into train, validation and testing arrays to use them during the training of the network.  \n",
    "you need to download notMNIST.data from this link ...\n",
    "\n",
    "https://drive.google.com/file/d/1ablp83xroWod-Mfr0dGlVM49YXNm32l7/view?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "from six.moves import cPickle as loader\n",
    "from six.moves import range\n",
    "\n",
    "data_file = 'notMNIST.data'\n",
    "\n",
    "with open(data_file, 'rb') as f:\n",
    "    data = loader.load(f)\n",
    "    train_dataset = data['train_dataset']\n",
    "    train_labels = data['train_labels']\n",
    "    valid_dataset = data['valid_dataset']\n",
    "    valid_labels = data['valid_labels']\n",
    "    test_dataset = data['test_dataset']\n",
    "    test_labels = data['test_labels']\n",
    "    del data \n",
    "\n",
    "    print('Training set', train_dataset.shape, train_labels.shape)\n",
    "    print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "    print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. Convince yourself about the data by showing few  images and printing the size of train/test/validation data arrays.[10 points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nlgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x13a8a4914a8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x13a8c547e10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x13a8d5b41d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    z=train_dataset[i]\n",
    "    # Plot the grid\n",
    "    plt.imshow(z)\n",
    "    plt.gray()\n",
    "    plt.show()\n",
    "\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b.  Reformat data to shape that's make it easy to train neural network models:\n",
    "-\t Reshape the data to a flat matrix.[5 points]. Hint: For example convert training dataset to have (200000, 784) shape, do the same for the rest of the arrays.\n",
    "-\t Reformat the output to 1-hot encodings. [5 points]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def flat_convert_2to1(dataset,length = 784):\n",
    "    converted_dataset = []\n",
    "    for img in dataset:\n",
    "        converted_dataset.append(img.reshape(length))\n",
    "    converted_dataset= np.array(converted_dataset)\n",
    "    return converted_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 784)\n",
      "Validation set (10000, 784)\n",
      "Test set (10000, 784)\n"
     ]
    }
   ],
   "source": [
    "converted_train_dataset = flat_convert_2to1(train_dataset)\n",
    "converted_valid_dataset = flat_convert_2to1(valid_dataset)\n",
    "converted_test_dataset = flat_convert_2to1(test_dataset)\n",
    "\n",
    "print('Training set', converted_train_dataset.shape)\n",
    "print('Validation set', converted_valid_dataset.shape)\n",
    "print('Test set', converted_test_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 10)\n",
      "Validation set (10000, 10)\n",
      "Test set (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "enc = OneHotEncoder()\n",
    "train_onehot_output = enc.fit_transform(train_labels.reshape(len(train_labels),1)).toarray()\n",
    "valid_onehot_output = enc.fit_transform(valid_labels.reshape(len(valid_labels),1)).toarray()\n",
    "test_onehot_output = enc.fit_transform(test_labels.reshape(len(test_labels),1)).toarray()\n",
    "\n",
    "print('Training set', train_onehot_output.shape)\n",
    "print('Validation set', valid_onehot_output.shape)\n",
    "print('Test set', test_onehot_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. Refer to the neural network lecture to answer the following questions (no need to code or to show experimental results for this question) [15 points]:\n",
    "- the best activation function.\n",
    "- the best initialization.\n",
    "- Best gradient descent update learning rule. \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Answer for part1.c :\n",
    "\n",
    "- **the best activation function**:  \n",
    "    The leaky relu activation fucntion\n",
    "- **the best initialization**:    \n",
    "    xavier initialization\n",
    "- **Best gradient descent update learning rule**:    \n",
    "    AdamOptimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d. Train a multi-layer neural network of one hidden layer neural network using the best: activation, initialization and gradient descent learning rule that you found in the previous question. \n",
    "Train for 15 epochs with a  batch size of 128 and pick the model that has less validation error and report the testing error for that model. Use the accuracy metric to measure the performance. [35 points]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "network is ready ...\n"
     ]
    }
   ],
   "source": [
    "# define the network toplogies, two hidden neural network with input diminsion of 784 \n",
    "# and output diminsion of 10\n",
    "\n",
    "n_hidden_1 = 256 \n",
    "n_input    = 784 \n",
    "n_classes  = 10  \n",
    "\n",
    "# placeholder for input and output, None: means it could be of any diminsion\n",
    "x = tf.placeholder(\"float\", [None, n_input], name = 'x')\n",
    "y = tf.placeholder(\"float\", [None, n_classes], name = 'y')\n",
    "phase = tf.placeholder(tf.bool, name='phase')\n",
    "\n",
    "initializer = tf.contrib.layers.xavier_initializer()\n",
    "\n",
    "# network parameters \n",
    "stddev = 0.1\n",
    "weights = {\n",
    "    'h1': tf.Variable(initializer((n_input,n_hidden_1))),\n",
    "    'out': tf.Variable(initializer((n_hidden_1,n_classes)))\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(initializer([n_hidden_1])),\n",
    "    'out': tf.Variable(initializer([n_classes]))\n",
    "}\n",
    "print (\"network is ready ...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def multilayer_perceptron(_X, _weights, _biases):\n",
    "    layer_1 = tf.nn.leaky_relu(tf.add(tf.matmul(_X, _weights['h1']), _biases['b1'])) \n",
    "    return (tf.matmul(layer_1, _weights['out']) + _biases['out'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ready to go ...\n"
     ]
    }
   ],
   "source": [
    "# build the network graph\n",
    "pred = multilayer_perceptron(x, weights, biases)\n",
    "\n",
    "# use softmax cross entropy as the loss(cost) function\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = pred, labels=y)) \n",
    "# define optimizer\n",
    "optm = tf.train.AdamOptimizer().minimize(cost)\n",
    "# measuring the accuracy\n",
    "corr = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "accr = tf.reduce_mean(tf.cast(corr, \"float\"))\n",
    "\n",
    "# initilize all the graph parameters.\n",
    "init = tf.global_variables_initializer()\n",
    "print (\"ready to go ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def train_MLP(save_path = \"/tmp_model/best_model.ckpt\"):\n",
    "    # Training params\n",
    "    training_epochs = 15\n",
    "    batch_size      = 128 # use mini-batch, we will disucss this next lecture\n",
    "    display_step    = 3 # display results every 4 epochs\n",
    "\n",
    "    #create the saver to save the best model.\n",
    "    saver = tf.train.Saver()\n",
    "    # launch the graph using Session()\n",
    "    sess = tf.Session()\n",
    "    # run with the initiales\n",
    "    sess.run(init)\n",
    "\n",
    "    # optimization code, we used here stochastic gradient descent SGD.\n",
    "    best_vaild_accs = 0\n",
    "    for epoch in range(training_epochs):\n",
    "        sample_num = len(converted_train_dataset)\n",
    "        avg_cost = 0.\n",
    "        total_batch = int(sample_num/batch_size)\n",
    "        # iterate\n",
    "        for i in range(total_batch):\n",
    "            start = i*batch_size\n",
    "            end = i*batch_size + batch_size-1\n",
    "            batch_xs, batch_ys = converted_train_dataset[start:end]\\\n",
    "                                , train_onehot_output[start:end]\n",
    "            feeds = {x: batch_xs, y: batch_ys}\n",
    "            sess.run(optm, feed_dict=feeds)\n",
    "            avg_cost += sess.run(cost, feed_dict=feeds) # to calculate the cost, you can skip\n",
    "        avg_cost = avg_cost / total_batch\n",
    "\n",
    "        #save the best neural network\n",
    "        feeds = {x: batch_xs, y: batch_ys}\n",
    "        train_acc = sess.run(accr, feed_dict=feeds)\n",
    "        feeds = {x: converted_valid_dataset, y: valid_onehot_output}\n",
    "        valid_acc = sess.run(accr, feed_dict=feeds)\n",
    "        if(valid_acc > best_vaild_accs): \n",
    "            best_vaild_accs = valid_acc\n",
    "            saver.save(sess,save_path)\n",
    "        # display\n",
    "        if (epoch+1) % display_step == 0:\n",
    "            print (\"Epoch: %03d/%03d cost: %.9f\" % (epoch+1, training_epochs, avg_cost))\n",
    "            print (\"Train Accuracy: %.3f\" % (train_acc))\n",
    "            print (\"Valid Accuracy: %.3f\" % (valid_acc))    \n",
    "\n",
    "    #get the best neural network\n",
    "    saver.restore(sess, save_path)\n",
    "    print (\"\\nThe best valid Accuracy: %.3f\" % (best_vaild_accs)) \n",
    "    feeds = {x: converted_test_dataset, y: test_onehot_output}\n",
    "    test_acc = sess.run(accr, feed_dict=feeds)   \n",
    "    print (\"Test Accuracy: %.3f\" % (test_acc))\n",
    "    print (\"Test error: %.3f\" % (1 - test_acc))\n",
    "    print (\"optimization finished\")\n",
    "    sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 003/015 cost: 0.358490747\n",
      "Train Accuracy: 0.850\n",
      "Valid Accuracy: 0.880\n",
      "Epoch: 006/015 cost: 0.298352629\n",
      "Train Accuracy: 0.882\n",
      "Valid Accuracy: 0.886\n",
      "Epoch: 009/015 cost: 0.261446621\n",
      "Train Accuracy: 0.898\n",
      "Valid Accuracy: 0.889\n",
      "Epoch: 012/015 cost: 0.233463887\n",
      "Train Accuracy: 0.913\n",
      "Valid Accuracy: 0.890\n",
      "Epoch: 015/015 cost: 0.210546389\n",
      "Train Accuracy: 0.937\n",
      "Valid Accuracy: 0.889\n",
      "\n",
      "The best valid Accuracy: 0.890\n",
      "Test Accuracy: 0.945\n",
      "Test error: 0.055\n",
      "optimization finished\n"
     ]
    }
   ],
   "source": [
    "train_MLP()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e. (This part for graduate students only) Which regularization method is the best to use, L2, dropout or Batch Normalization (justify your answer experimentally). does the batch normalization help in converging the network faster ? [30 points]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer:\n",
    "** As shown below, I think the dropout is the best regularzion method**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#L2\n",
    "\n",
    "# Original cost function\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = pred, labels=y)) \n",
    "# add l2 regulazation\n",
    "beta = 0.01\n",
    "regularizer = tf.nn.l2_loss(weights['h1']) + tf.nn.l2_loss(weights['out'])\n",
    "cost = tf.reduce_mean(cost + beta * regularizer)\n",
    "optm = tf.train.AdamOptimizer().minimize(cost)\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 003/015 cost: 0.728255881\n",
      "Train Accuracy: 0.819\n",
      "Valid Accuracy: 0.830\n",
      "Epoch: 006/015 cost: 0.719555619\n",
      "Train Accuracy: 0.819\n",
      "Valid Accuracy: 0.830\n",
      "Epoch: 009/015 cost: 0.716368231\n",
      "Train Accuracy: 0.819\n",
      "Valid Accuracy: 0.834\n",
      "Epoch: 012/015 cost: 0.714818664\n",
      "Train Accuracy: 0.827\n",
      "Valid Accuracy: 0.835\n",
      "Epoch: 015/015 cost: 0.713945810\n",
      "Train Accuracy: 0.819\n",
      "Valid Accuracy: 0.835\n",
      "\n",
      "The best valid Accuracy: 0.835\n",
      "Test Accuracy: 0.901\n",
      "Test error: 0.099\n",
      "optimization finished\n"
     ]
    }
   ],
   "source": [
    "train_MLP()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ready to go ...\n"
     ]
    }
   ],
   "source": [
    "#dropout\n",
    "# prepare the graph\n",
    "keep_prob = tf.placeholder(\"float\")\n",
    "save_path = \"/tmp_model/best_model.ckpt\"\n",
    "def multilayer_perceptron_dropout(_X, _weights, _biases):\n",
    "    layer_1 = tf.nn.leaky_relu(tf.add(tf.matmul(_X, _weights['h1']), _biases['b1'])) \n",
    "    drop_out = tf.nn.dropout(layer_1, keep_prob)\n",
    "    return (tf.matmul(layer_1, _weights['out']) + _biases['out'])\n",
    "\n",
    "# build the network graph\n",
    "pred_drop_out = multilayer_perceptron_dropout(x, weights, biases)\n",
    "\n",
    "# use softmax cross entropy as the loss(cost) function\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = pred_drop_out, labels=y)) \n",
    "\n",
    "corr = tf.equal(tf.argmax(pred_drop_out, 1), tf.argmax(y, 1))\n",
    "accr = tf.reduce_mean(tf.cast(corr, \"float\"))\n",
    "# define optimizer\n",
    "optm = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "# initilize all the graph parameters.\n",
    "init = tf.global_variables_initializer()\n",
    "print (\"ready to go ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 003/015 cost: 0.357939605\n",
      "Train Accuracy: 0.850\n",
      "Valid Accuracy: 0.880\n",
      "Epoch: 006/015 cost: 0.299080095\n",
      "Train Accuracy: 0.874\n",
      "Valid Accuracy: 0.887\n",
      "Epoch: 009/015 cost: 0.263168589\n",
      "Train Accuracy: 0.898\n",
      "Valid Accuracy: 0.888\n",
      "Epoch: 012/015 cost: 0.235682250\n",
      "Train Accuracy: 0.913\n",
      "Valid Accuracy: 0.886\n",
      "Epoch: 015/015 cost: 0.212868331\n",
      "Train Accuracy: 0.921\n",
      "Valid Accuracy: 0.886\n",
      "\n",
      "The best valid Accuracy: 0.888\n",
      "Test Accuracy: 0.947\n",
      "Test error: 0.053\n",
      "optimization finished\n"
     ]
    }
   ],
   "source": [
    "# Training params\n",
    "training_epochs = 15\n",
    "batch_size      = 128 # use mini-batch, we will disucss this next lecture\n",
    "display_step    = 3 # display results every 4 epochs\n",
    "\n",
    "#create the saver to save the best model.\n",
    "saver = tf.train.Saver()\n",
    "# launch the graph using Session()\n",
    "sess = tf.Session()\n",
    "# run with the initiales\n",
    "sess.run(init)\n",
    "\n",
    "# optimization code, we used here stochastic gradient descent SGD.\n",
    "best_vaild_accs = 0\n",
    "for epoch in range(training_epochs):\n",
    "    sample_num = len(converted_train_dataset)\n",
    "    avg_cost = 0.\n",
    "    total_batch = int(sample_num/batch_size)\n",
    "    # iterate\n",
    "    for i in range(total_batch):\n",
    "        start = i*batch_size\n",
    "        end = i*batch_size + batch_size-1\n",
    "        batch_xs, batch_ys = converted_train_dataset[start:end]\\\n",
    "                            , train_onehot_output[start:end]\n",
    "        feeds = {x: batch_xs, y: batch_ys, keep_prob : 0.5}\n",
    "        sess.run(optm, feed_dict=feeds)\n",
    "        avg_cost += sess.run(cost, feed_dict=feeds) # to calculate the cost, you can skip\n",
    "    avg_cost = avg_cost / total_batch\n",
    "\n",
    "    #save the best neural network\n",
    "    feeds = {x: batch_xs, y: batch_ys, keep_prob : 0.5}\n",
    "    train_acc = sess.run(accr, feed_dict=feeds)\n",
    "    feeds = {x: converted_valid_dataset, y: valid_onehot_output, keep_prob : 1}\n",
    "    valid_acc = sess.run(accr, feed_dict=feeds)\n",
    "    if(valid_acc > best_vaild_accs): \n",
    "        best_vaild_accs = valid_acc\n",
    "        saver.save(sess,save_path)\n",
    "    # display\n",
    "    if (epoch+1) % display_step == 0:\n",
    "        print (\"Epoch: %03d/%03d cost: %.9f\" % (epoch+1, training_epochs, avg_cost))\n",
    "        print (\"Train Accuracy: %.3f\" % (train_acc))\n",
    "        print (\"Valid Accuracy: %.3f\" % (valid_acc))    \n",
    "\n",
    "#get the best neurla network\n",
    "saver.restore(sess, save_path)\n",
    "print (\"\\nThe best valid Accuracy: %.3f\" % (best_vaild_accs)) \n",
    "feeds = {x: converted_test_dataset, y: test_onehot_output, keep_prob : 1}\n",
    "test_acc = sess.run(accr, feed_dict=feeds)   \n",
    "print (\"Test Accuracy: %.3f\" % (test_acc))\n",
    "print (\"Test error: %.3f\" % (1 - test_acc))\n",
    "\n",
    "print (\"optimization finished\")\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ready to go ...\n"
     ]
    }
   ],
   "source": [
    "# Batch\n",
    "# prepare the graph\n",
    "save_path = \"/tmp_model/best_model.ckpt\"\n",
    "\n",
    "def multilayer_perceptron_batch_norm(_X, _weights, _biases, phase):\n",
    "    layer_1 = tf.add(tf.matmul(_X, _weights['h1']), _biases['b1'])\n",
    "    layer_1_batch = tf.nn.leaky_relu(tf.contrib.layers.batch_norm(layer_1, center=True, scale=True, is_training=phase)) \n",
    "    return (tf.matmul(layer_1_batch, _weights['out']) + _biases['out'])\n",
    "\n",
    "# build the network graph\n",
    "pred_drop_batch = multilayer_perceptron_batch_norm(x, weights, biases, phase)\n",
    "\n",
    "# use softmax cross entropy as the loss(cost) function\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = pred_drop_batch, labels=y)) \n",
    "\n",
    "corr = tf.equal(tf.argmax(pred_drop_batch, 1), tf.argmax(y, 1))\n",
    "accr = tf.reduce_mean(tf.cast(corr, \"float\"))\n",
    "# define optimizer\n",
    "optm = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "# initilize all the graph parameters.\n",
    "init = tf.global_variables_initializer()\n",
    "print (\"ready to go ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 003/015 cost: 0.370271618\n",
      "Train Accuracy: 0.843\n",
      "Valid Accuracy: 0.803\n",
      "Epoch: 006/015 cost: 0.307809553\n",
      "Train Accuracy: 0.874\n",
      "Valid Accuracy: 0.783\n",
      "Epoch: 009/015 cost: 0.269975222\n",
      "Train Accuracy: 0.906\n",
      "Valid Accuracy: 0.770\n",
      "Epoch: 012/015 cost: 0.241041594\n",
      "Train Accuracy: 0.913\n",
      "Valid Accuracy: 0.763\n",
      "Epoch: 015/015 cost: 0.217135365\n",
      "Train Accuracy: 0.913\n",
      "Valid Accuracy: 0.764\n",
      "\n",
      "The best valid Accuracy: 0.803\n",
      "Test Accuracy: 0.871\n",
      "Test error: 0.129\n",
      "optimization finished\n"
     ]
    }
   ],
   "source": [
    "# Training params\n",
    "training_epochs = 15\n",
    "batch_size      = 128 # use mini-batch, we will disucss this next lecture\n",
    "display_step    = 3 # display results every 4 epochs\n",
    "\n",
    "#create the saver to save the best model.\n",
    "saver = tf.train.Saver()\n",
    "# launch the graph using Session()\n",
    "sess = tf.Session()\n",
    "# run with the initiales\n",
    "sess.run(init)\n",
    "\n",
    "# optimization code, we used here stochastic gradient descent SGD.\n",
    "best_vaild_accs = 0\n",
    "for epoch in range(training_epochs):\n",
    "    sample_num = len(converted_train_dataset)\n",
    "    avg_cost = 0.\n",
    "    total_batch = int(sample_num/batch_size)\n",
    "    # iterate\n",
    "    for i in range(total_batch):\n",
    "        start = i*batch_size\n",
    "        end = i*batch_size + batch_size-1\n",
    "        batch_xs, batch_ys = converted_train_dataset[start:end]\\\n",
    "                            , train_onehot_output[start:end]\n",
    "        feeds = {x: batch_xs, y: batch_ys, phase : True}\n",
    "        sess.run(optm, feed_dict=feeds)\n",
    "        avg_cost += sess.run(cost, feed_dict=feeds) # to calculate the cost, you can skip\n",
    "    avg_cost = avg_cost / total_batch\n",
    "\n",
    "    #save the best neural network \n",
    "    # display\n",
    "    if (epoch+1) % display_step == 0:\n",
    "        print (\"Epoch: %03d/%03d cost: %.9f\" % (epoch+1, training_epochs, avg_cost))\n",
    "        feeds = {x: batch_xs, y: batch_ys, phase : True}\n",
    "        train_acc = sess.run(accr, feed_dict=feeds)\n",
    "        print (\"Train Accuracy: %.3f\" % (train_acc))\n",
    "        feeds = {x: converted_valid_dataset, y: valid_onehot_output, phase : False}\n",
    "        valid_acc = sess.run(accr, feed_dict=feeds)\n",
    "        print (\"Valid Accuracy: %.3f\" % (valid_acc))    \n",
    "        if(valid_acc > best_vaild_accs): \n",
    "            best_vaild_accs = valid_acc\n",
    "            saver.save(sess,save_path)\n",
    "\n",
    "#get the best neural network\n",
    "saver.restore(sess, save_path)\n",
    "print (\"\\nThe best valid Accuracy: %.3f\" % (best_vaild_accs)) \n",
    "feeds = {x: converted_test_dataset, y: test_onehot_output, phase : False}\n",
    "test_acc = sess.run(accr, feed_dict=feeds)   \n",
    "print (\"Test Accuracy: %.3f\" % (test_acc))\n",
    "print (\"Test error: %.3f\" % (1 - test_acc))\n",
    "\n",
    "print (\"optimization finished\")\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(This whole part is for graduate students only) In this part you are going to train a convolutional neural network on notMNIST dataset, refer to part1 to use same training, validation and testing datasets. \n",
    "\n",
    "Start by reading the Deep MNIST for Experts tutorial on the tensorflow website (https://www.tensorflow.org/get_started/mnist/pros). \n",
    "Go through the examples provided in this tutorial. \n",
    "\n",
    "\n",
    "in here we are going to build a different network. Itâ€™d be helpful to quickly go through the examples provided in the tutorial.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. Define the model as indicated in the code below. The model is defined as follows:\n",
    "\n",
    "- An input that is 728 dimensional vector.\n",
    "- Reshape the input as 28x28x1 images (only 1 because they are grey scale)\n",
    "- A convolutional layer with 25 filters of shape 12x12x1 and a ReLU non-linearity (with stride (2, 2) and no padding)\n",
    "- A convolutional layer with 64 filters of shape 5x5x25 and a ReLU non-linearity (with stride (1, 2) and padding to maintain size)\n",
    "- A max_pooling layer of shape 2x2\n",
    "- A fully connected layer taking all the outputs of the max_pooling layer to 1024 units and ReLU nonlinearity\n",
    "- A fully connected layer taking 1024 units to 10 no activation function (the softmax non-linearity will be included in the loss function rather than in the model) [15 points]\n",
    "\n",
    "Hint: start from known architecture then modify the code to match the numbers listed above, you might need to have flat layer that flatten max pool layer data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv2d(x, W, strides = [1, 2, 2, 1], padding='SAME'):\n",
    "    \"\"\"conv2d returns a 2d convolution layer with full stride.\"\"\"\n",
    "    return tf.nn.conv2d(x, W, strides, padding)\n",
    "\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    \"\"\"max_pool_2x2 downsamples a feature map by 2X.\"\"\"\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "def weight_variable(shape):\n",
    "    \"\"\"weight_variable generates a weight variable of a given shape.\"\"\"\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "\n",
    "def bias_variable(shape):\n",
    "    \"\"\"bias_variable generates a bias variable of a given shape.\"\"\"\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def deepnn(x):\n",
    "    \"\"\"deepnn builds the graph for a deep net for classifying digits.\n",
    "    Args:\n",
    "    x: an input tensor with the dimensions (N_examples, 784), where 784 is the\n",
    "    number of pixels in a standard MNIST image.\n",
    "    Returns:\n",
    "    A tuple (y, keep_prob). y is a tensor of shape (N_examples, 10), with values\n",
    "    equal to the logits of classifying the digit into one of 10 classes (the\n",
    "    digits 0-9). keep_prob is a scalar placeholder for the probability of\n",
    "    dropout.\n",
    "    \"\"\"\n",
    "    # Reshape to use within a convolutional neural net.\n",
    "    # Last dimension is for \"features\" - there is only one here, since images are\n",
    "    # grayscale -- it would be 3 for an RGB image, 4 for RGBA, etc.\n",
    "    with tf.name_scope('reshape'):\n",
    "        x_image = tf.reshape(x, [-1, 28, 28, 1])\n",
    "   # print(\"x_image\", x_image.shape)\n",
    "    # First convolutional layer - maps one grayscale image to  feature maps.\n",
    "    with tf.name_scope('conv1'):\n",
    "        W_conv1 = weight_variable([12, 12, 1, 25])\n",
    "        b_conv1 = bias_variable([25])\n",
    "        h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1,strides = [1,2,2,1], padding = 'VALID') + b_conv1)\n",
    "    #print(\"h_conv1\", h_conv1.shape)\n",
    "    # Second convolutional layer -- maps 25 feature maps to 64.\n",
    "    with tf.name_scope('conv2'):\n",
    "        W_conv2 = weight_variable([5, 5, 25, 64])\n",
    "        b_conv2 = bias_variable([64])\n",
    "        h_conv2 = tf.nn.relu(conv2d(h_conv1, W_conv2,strides = [1,1,2,1], padding = 'SAME') + b_conv2)\n",
    "    #print(\"h_conv2\", h_conv2.shape)\n",
    "    # Second pooling layer.\n",
    "    with tf.name_scope('pool2'):\n",
    "        h_pool2 = max_pool_2x2(h_conv2)\n",
    "    #print(\"h_pool2\", h_pool2.shape)\n",
    "    # Fully connected layer 1 -- after 2 round of downsampling, our 28x28 image\n",
    "    # is down to 5x3x64 feature maps -- maps this to 1024 features.\n",
    "    with tf.name_scope('fc1'):\n",
    "        W_fc1 = weight_variable([5 * 3 * 64, 1024])\n",
    "        b_fc1 = bias_variable([1024])\n",
    "\n",
    "        h_pool2_flat = tf.reshape(h_pool2, [-1, 5 * 3 * 64])\n",
    "        h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "\n",
    "\n",
    "    # Map the 1024 features to 10 classes, one for each digit\n",
    "    with tf.name_scope('fc2'):\n",
    "        W_fc2 = weight_variable([1024, 10])\n",
    "        b_fc2 = bias_variable([10])\n",
    "\n",
    "        y_conv = tf.matmul(h_fc1, W_fc2) + b_fc2\n",
    "    return y_conv, W_conv1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. write the code to train the model written in (a), train for 15 epochs with a  batch size of 128. \n",
    "\n",
    "Loss Function, Accuracy and Training Algorithm\n",
    "\n",
    "- You will use the cross entropy loss function. The loss function is called tf.nn.cross_entropy_with_logits in tensorflow\n",
    "- Accuray is simply defined as the fraction of data correctly classified\n",
    "- For training you should use the AdamOptimizer (read the documentation) and initially pick the learning rate to be 0.05 (if this learning rate does not work, pick different learning rate) with decay step of 0.95 every 2000 iterations as showen in the code below. You are encouraged, to experiment with other optimisation procedures and learning rates. [25 points]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# and output diminsion of 10\n",
    "\n",
    "n_input    = 784 \n",
    "n_classes  = 10  \n",
    "\n",
    "# placeholder for input and output, None: means it could be of any diminsion\n",
    "x = tf.placeholder(\"float\", [None, n_input])\n",
    "y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "y_conv, W_conv= deepnn(x)\n",
    "\n",
    "# We'll use the cross entropy loss function \n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = y_conv, labels= y))\n",
    "\n",
    "# And classification accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# And the Adam optimiser\n",
    "global_step = tf.Variable(0)\n",
    "learning_rate = tf.train.exponential_decay(0.05, global_step, 2000, 0.95)\n",
    "optimizer = tf.train.AdagradOptimizer(learning_rate).minimize(cross_entropy, global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From E:\\Anaconda\\Anaconda_entity\\lib\\site-packages\\tensorflow\\python\\util\\tf_should_use.py:107: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Epoch: 003/015 cost: 0.242161549\n",
      "Train Accuracy: 0.898\n",
      "Valid Accuracy: 0.894\n",
      "Epoch: 006/015 cost: 0.178617277\n",
      "Train Accuracy: 0.945\n",
      "Valid Accuracy: 0.902\n",
      "Epoch: 009/015 cost: 0.137836489\n",
      "Train Accuracy: 0.953\n",
      "Valid Accuracy: 0.904\n",
      "Epoch: 012/015 cost: 0.108114066\n",
      "Train Accuracy: 0.953\n",
      "Valid Accuracy: 0.906\n",
      "Epoch: 015/015 cost: 0.085890086\n",
      "Train Accuracy: 0.961\n",
      "Valid Accuracy: 0.908\n",
      "\n",
      "The best valid Accuracy: 0.908\n",
      "Test Accuracy: 0.961\n",
      "Test error: 0.039\n",
      "optimization finished\n"
     ]
    }
   ],
   "source": [
    " # Start a tf session and run the optimisation algorithm\n",
    "sess = tf.Session()\n",
    "sess.run(tf.initialize_all_variables())\n",
    "save_path = \"/tmp_model/best_model.ckpt\"\n",
    "# use \n",
    "#TODO\n",
    "# Write the optimisation code here\n",
    "training_epochs = 15\n",
    "batch_size      = 128 # use mini-batch, we will disucss this next lecture\n",
    "display_step    = 3 # display results every 4 epochs\n",
    "\n",
    "\n",
    "#create the saver to save the best model.\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# optimization code, we used here stochastic gradient descent SGD.\n",
    "best_vaild_accs = 0\n",
    "for epoch in range(training_epochs):\n",
    "    sample_num = len(converted_train_dataset)\n",
    "    avg_cost = 0.\n",
    "    total_batch = int(sample_num/batch_size)\n",
    "    # iterate\n",
    "    for i in range(total_batch):\n",
    "        start = i*batch_size\n",
    "        end = i*batch_size + batch_size-1\n",
    "        batch_xs, batch_ys = converted_train_dataset[start:end]\\\n",
    "                            , train_onehot_output[start:end]\n",
    "        feeds = {x: batch_xs, y: batch_ys}\n",
    "        sess.run(optimizer, feed_dict=feeds)\n",
    "        avg_cost += sess.run(cross_entropy, feed_dict=feeds) # to calculate the cost, you can skip\n",
    "    avg_cost = avg_cost / total_batch\n",
    "\n",
    "    #save the best neural network\n",
    "    feeds = {x: batch_xs, y: batch_ys}\n",
    "    train_acc = sess.run(accuracy, feed_dict=feeds)\n",
    "    feeds = {x: converted_valid_dataset, y: valid_onehot_output}\n",
    "    valid_acc = sess.run(accuracy, feed_dict=feeds)\n",
    "    if(valid_acc > best_vaild_accs): \n",
    "        best_vaild_accs = valid_acc\n",
    "        saver.save(sess,save_path)\n",
    "    # display\n",
    "    if (epoch+1) % display_step == 0:\n",
    "        print (\"Epoch: %03d/%03d cost: %.9f\" % (epoch+1, training_epochs, avg_cost))\n",
    "        print (\"Train Accuracy: %.3f\" % (train_acc))\n",
    "        print (\"Valid Accuracy: %.3f\" % (valid_acc))    \n",
    "\n",
    "#get the best neural network\n",
    "saver.restore(sess, save_path)\n",
    "print (\"\\nThe best valid Accuracy: %.3f\" % (best_vaild_accs)) \n",
    "feeds = {x: converted_test_dataset, y: test_onehot_output}\n",
    "test_acc = sess.run(accuracy, feed_dict=feeds)   \n",
    "print (\"Test Accuracy: %.3f\" % (test_acc))\n",
    "print (\"Test error: %.3f\" % (1 - test_acc))\n",
    "print (\"optimization finished\")\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. [Extra credits] Find better convolutional neural network architecture that give better results (at least enhancment of 3.0%) than the one built in part b (prove experimentally). [15 points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d. [Extra credits] Visualize all the 32 filters in the first convolution layer. Each of shape 12x12x1, they might be viewed as greyscale images. [15 points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12, 12, 1, 25)\n"
     ]
    }
   ],
   "source": [
    "initialization = tf.initialize_all_variables()\n",
    "#strat section\n",
    "sess = tf.Session()\n",
    "sess.run(initialization)\n",
    "\n",
    "# Retrieve the values of the  first convolution weights from TensorFlow.\n",
    "weights = sess.run(W_conv)\n",
    "print(weights.shape)\n",
    "input_channel=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAb8AAAFbCAYAAABFxoDmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n6pWsTToJ9AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x13a9b0d8d68>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Get the lowest and highest values for the weights.\n",
    "w_min = np.min(weights)\n",
    "w_max = np.max(weights)\n",
    "\n",
    "#get number of filters used in the conv. layer.\n",
    "num_filters = weights.shape[3]\n",
    "\n",
    "# Number of grids to plot.\n",
    "num_grids = math.ceil(math.sqrt(num_filters))\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(num_grids, num_grids)\n",
    "fig.set_size_inches(8, 6)\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    \n",
    "    if i<num_filters:        \n",
    "        image = weights[:, :, input_channel, i]\n",
    "        ax.imshow(image, vmin=w_min, vmax=w_max,\n",
    "                  interpolation='nearest', cmap='gray')\n",
    "\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
